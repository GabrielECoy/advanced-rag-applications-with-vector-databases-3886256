{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./Big Star Collectibles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 139, which is longer than the specified 128\n",
      "Created a chunk of size 151, which is longer than the specified 128\n",
      "Created a chunk of size 151, which is longer than the specified 128\n",
      "Created a chunk of size 139, which is longer than the specified 128\n",
      "Created a chunk of size 130, which is longer than the specified 128\n",
      "Created a chunk of size 188, which is longer than the specified 128\n",
      "Created a chunk of size 130, which is longer than the specified 128\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(data_dir)\n",
    "file_texts = []\n",
    "for file in files:\n",
    "    with open(f\"{data_dir}/{file}\") as f:\n",
    "        file_text = f.read()\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=128, chunk_overlap=32,\n",
    "    )\n",
    "    texts = text_splitter.split_text(file_text)\n",
    "    for i, chunked_text in enumerate(texts):\n",
    "        file_texts.append(Document(page_content=chunked_text,metadata={ \n",
    "                    \"doc_title\": file.split(\".\")[0], \n",
    "                    \"chunk_num\": i}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12039/3428753526.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings() # embed your data\n",
      "/tmp/ipykernel_12039/3428753526.py:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings() # embed your data\n",
      "/workspaces/advanced-rag-applications-with-vector-databases-3886256/ch1/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings() # embed your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the embedded data into a vector database\n",
    "vector_store = FAISS.from_documents(\n",
    "    file_texts,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# I added a .env file in the /workspaces/advanced-rag-applications-with-vector-databases-3886256/chapter_1 folder with the OPENAI_API_KEY value\n",
    "# It is not saved in git because .gitignore has .env in it\n",
    "# Also changes the instanciation of OpenAI below to point to Azure OpenAI endpoint\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# added this to point to Azure OpenAI endpoint (see cell above for more info on .env file and OPENAI_API_KEY)\n",
    "# Also added reference to 4o-mini model because I was getting 'Unknown model: gpt-3.5-turbo-instruct', perhaps\n",
    "# that the model the older versions of the toolds refered in the req of this example are using by default\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "llm = OpenAI(base_url=endpoint, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template=\"\"\"You are a helpful assistant. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Cite your sources.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"When did Big Star Collectibles Launch? Cite where you found this information.\") # critical line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Big Star Collectibles officially launched in 2014. [Our Story, chunk_num=1]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first answer was \"' Big Star Collectibles officially launched in 2014. This information can be found in the \"Our Story\" document.<|fim_suffix|>'\"\n",
    "\n",
    "I resolved the problem by switching from gpt-4o-mini to gpt-4o, previously I tried another solution that did not work\n",
    "\n",
    "Ignored solution proposed by Copilot:\n",
    "You are getting \"fims_suffix\" (or similar placeholder text) instead of an actual document reference in cell 17 because the chain and prompt do not explicitly instruct the language model to include the metadata (such as document title or chunk number) from the retrieved documents in the context passed to the LLM.\n",
    "\n",
    "By default, the retriever returns only the text content, not the metadata. If you want the LLM to cite sources, you need to:\n",
    "1. Modify the prompt so it includes metadata (like `doc_title` and `chunk_num`) in the context.\n",
    "2. Adjust how the context is constructed so that each chunk includes its source information.\n",
    "\n",
    "To fix this, update the code that builds the context for the prompt so it formats each chunk with its metadata, for example:\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata.get('doc_title', 'unknown')} (chunk {doc.metadata.get('chunk_num', '?')}):\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "# Then, in your chain, use a RunnableLambda to format the context:\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(format_docs),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "This way, the LLM will see the source information in the context and can cite it in its answer. If you want, I can update your notebook to implement this fix.\n",
    "\n",
    "\"fim_suffix\" is not a standard output or variable from LangChain, OpenAI, or your notebook code. It is likely a hallucinated or placeholder string generated by the language model (LLM) when it does not have access to real citation metadata in the context.\n",
    "\n",
    "This happens because:\n",
    "\n",
    "The LLM is prompted to \"Cite your sources,\" but the context it receives does not include explicit source information (like document titles or chunk numbers).\n",
    "Without this information, the LLM may invent a placeholder like \"fim_suffix\" or similar, as it tries to fulfill the instruction to cite a source.\n",
    "To get real citations, you must format the context passed to the LLM to include the actual metadata (such as document title and chunk number) for each chunk. Otherwise, the LLM will not know the true source and may generate made-up references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata.get('doc_title', 'unknown')} (chunk {doc.metadata.get('chunk_num', '?')}):\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "# Then, in your chain, use a RunnableLambda to format the context:\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(format_docs),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, the LLM will see the source information in the context and can cite it in its answer. If you want, I can update your notebook to implement this fix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ch1 (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
