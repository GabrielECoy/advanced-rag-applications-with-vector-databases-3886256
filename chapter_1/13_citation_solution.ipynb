{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./Big Star Collectibles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 139, which is longer than the specified 128\n",
      "Created a chunk of size 151, which is longer than the specified 128\n",
      "Created a chunk of size 151, which is longer than the specified 128\n",
      "Created a chunk of size 139, which is longer than the specified 128\n",
      "Created a chunk of size 130, which is longer than the specified 128\n",
      "Created a chunk of size 188, which is longer than the specified 128\n",
      "Created a chunk of size 130, which is longer than the specified 128\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(data_dir)\n",
    "file_texts = []\n",
    "for file in files:\n",
    "    with open(f\"{data_dir}/{file}\") as f:\n",
    "        file_text = f.read()\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=128, chunk_overlap=32,\n",
    "    )\n",
    "    texts = text_splitter.split_text(file_text)\n",
    "    for i, chunked_text in enumerate(texts):\n",
    "        file_texts.append(Document(page_content=chunked_text,metadata={ \n",
    "                    \"doc_title\": file.split(\".\")[0], \n",
    "                    \"chunk_num\": i}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21387/3428753526.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings() # embed your data\n",
      "/tmp/ipykernel_21387/3428753526.py:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings() # embed your data\n",
      "/workspaces/advanced-rag-applications-with-vector-databases-3886256/ch1/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings() # embed your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the embedded data into a vector database\n",
    "vector_store = FAISS.from_documents(\n",
    "    file_texts,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# I added a .env file in the /workspaces/advanced-rag-applications-with-vector-databases-3886256/chapter_1 folder with the OPENAI_API_KEY value\n",
    "# It is not saved in git because .gitignore has .env in it\n",
    "# Also changes the instanciation of OpenAI below to point to Azure OpenAI endpoint\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# added this to point to Azure OpenAI endpoint (see cell above for more info on .env file and OPENAI_API_KEY)\n",
    "# Also added reference to 4o-mini model because I was getting 'Unknown model: gpt-3.5-turbo-instruct', perhaps\n",
    "# that the model the older versions of the toolds refered in the req of this example are using by default\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "llm = OpenAI(base_url=endpoint, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template:  You are a helpful assistant. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Cite your sources.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n",
      "Prompt:  input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are a helpful assistant. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nCite your sources.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template=\"\"\"You are a helpful assistant. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Cite your sources.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(\"Template: \",template)\n",
    "print(\"Prompt: \",prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# create a chain that combines the retriever, prompt, and llm\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asked Cloud to generate this function to run the chain step by step with inspection points\n",
    "# This function prints detailed information at each step, showing you exactly what's happening:\n",
    "# Step 1: The initial question input\n",
    "# Step 2: Retrieved context documents and the question dict\n",
    "# Step 3: The formatted prompt after template substitution\n",
    "# Step 4: The raw LLM output\n",
    "# Step 5: The final parsed string output\n",
    "\n",
    "def run_chain_with_inspection(question_input):\n",
    "   \n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 1: Initial Input\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Input: {question_input}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 1: Create the dictionary with context and question\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 2: Retrieve Context & Prepare Dict\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    context_docs = retriever.invoke(question_input)\n",
    "    print(f\"Retrieved {len(context_docs)} documents\")\n",
    "    print(f\"Context (first 200 chars): {str(context_docs)[:200]}...\")\n",
    "    \n",
    "    # RunnablePassthrough() just passes the input through\n",
    "    question = question_input\n",
    "    print(f\"Question: {question}\")\n",
    "    \n",
    "    # Create the dictionary\n",
    "    step1_output = {\n",
    "        \"context\": context_docs,\n",
    "        \"question\": question\n",
    "    }\n",
    "    print(f\"\\nStep 1 Output (dict keys): {step1_output.keys()}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Format with prompt template\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 3: Format Prompt\")\n",
    "    print(\"=\" * 60)\n",
    "    step2_output = prompt.invoke(step1_output)\n",
    "    print(f\"Formatted Prompt Type: {type(step2_output)}\")\n",
    "    print(f\"Formatted Prompt:\\n{step2_output}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Send to LLM\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 4: LLM Generation\")\n",
    "    print(\"=\" * 60)\n",
    "    step3_output = llm.invoke(step2_output)\n",
    "    print(f\"LLM Output Type: {type(step3_output)}\")\n",
    "    print(f\"LLM Output : {step3_output}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Parse output to string\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 5: Parse to String\")\n",
    "    print(\"=\" * 60)\n",
    "    parser = StrOutputParser()\n",
    "    final_output = parser.invoke(step3_output)\n",
    "    print(f\"Final Output Type: {type(final_output)}\")\n",
    "    print(f\"Final Output:\\n{final_output}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"CHAIN COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Initial Input\n",
      "============================================================\n",
      "Input: When did Big Star Collectibles Launch? Cite where you found this information.\n",
      "\n",
      "============================================================\n",
      "STEP 2: Retrieve Context & Prepare Dict\n",
      "============================================================\n",
      "Retrieved 4 documents\n",
      "Context (first 200 chars): [Document(metadata={'doc_title': 'Our Story', 'chunk_num': 1}, page_content='Launched officially in 2014, Saura was determined to create high-quality trading cards that were desirable and valuable for...\n",
      "Question: When did Big Star Collectibles Launch? Cite where you found this information.\n",
      "\n",
      "Step 1 Output (dict keys): dict_keys(['context', 'question'])\n",
      "\n",
      "============================================================\n",
      "STEP 3: Format Prompt\n",
      "============================================================\n",
      "Formatted Prompt Type: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "Formatted Prompt:\n",
      "messages=[HumanMessage(content=\"You are a helpful assistant. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nCite your sources.\\nQuestion: When did Big Star Collectibles Launch? Cite where you found this information. \\nContext: [Document(metadata={'doc_title': 'Our Story', 'chunk_num': 1}, page_content='Launched officially in 2014, Saura was determined to create high-quality trading cards that were desirable and valuable for the collecting community. Besides monthly releases for the casual collector, Big Star Collectibles also releases limited editions and one-of-a-kind items. \\\\n\\\\nBig Star Collectibles has grown over the years to include memorabilia, contests, events, appraisals, and consultation services.'), Document(metadata={'doc_title': 'Our Story', 'chunk_num': 0}, page_content='Our story began at the International Arts Conference in 2013. Our founder, Saura Chen, a trained photographer, captured a series of candid images and portraits of the keynote speaker and presenters at the event, and provided print copies of the photographs to attendees at the end of the day. When she overheard a group of attendees attempting to secure autographs from the presenters and negotiating photo trades, the seeds for Big Star Collectibles were planted.'), Document(metadata={'doc_title': 'FAQ', 'chunk_num': 1}, page_content='How can I find out how much my Big Star Collectibles product is worth? \\\\nThere are a few ways to find out what your collectible is worth:\\\\nHire a Big Star Collectibles expert to conduct a formal appraisal. Depending on your needs, this can be a simple retail estimate, or a detailed analysis of its insurance replacement value, or a collectible’s projected value over a determined time frame.\\\\nKeep up with overall value trends on our website and newsletters.\\\\nPost about it in our online user’s forum. While this isn’t a precise measure of worth, it may give you an idea of its desirability among your peers.'), Document(metadata={'doc_title': 'What We Do', 'chunk_num': 0}, page_content='We go to the far reaches of the galaxy to bring top quality, authentic, and rare collectibles right to your door. \\\\n\\\\nDesign and Sell\\\\nThe most apparent of our activities is designing and selling collectibles that reflect what our customers enjoy and want. Our team of product designers analyze and speculate new collectibles based on customer and market feedback. And we love surprising you.\\\\n\\\\nSearch and Broker\\\\nFor a fee, our experts can assist you in finding a particular Big Star Collectibles item that you have been looking for. Big Star Collectibles can also broker sales and trades among our customers.')] \\nAnswer:\", additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "============================================================\n",
      "STEP 4: LLM Generation\n",
      "============================================================\n",
      "LLM Output Type: <class 'str'>\n",
      "LLM Output :  Big Star Collectibles officially launched in 2014. [Our Story, chunk_num=1]\n",
      "\n",
      "============================================================\n",
      "STEP 5: Parse to String\n",
      "============================================================\n",
      "Final Output Type: <class 'str'>\n",
      "Final Output:\n",
      " Big Star Collectibles officially launched in 2014. [Our Story, chunk_num=1]\n",
      "\n",
      "============================================================\n",
      "CHAIN COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "doitstepbystep = True\n",
    "question = \"When did Big Star Collectibles Launch? Cite where you found this information.\"\n",
    "\n",
    "if doitstepbystep:\n",
    "    print\n",
    "    response = run_chain_with_inspection(question)\n",
    "else:\n",
    "    print (\"Using the Langchain runnable chain\")\n",
    "    response = chain.invoke(question) # critical line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Big Star Collectibles officially launched in 2014. [Our Story, chunk_num=1]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first answer was \"' Big Star Collectibles officially launched in 2014. This information can be found in the \"Our Story\" document.<|fim_suffix|>'\"\n",
    "\n",
    "I resolved the problem by switching from gpt-4o-mini to gpt-4o, previously I tried another solution that did not work\n",
    "\n",
    "Ignored solution proposed by Copilot:\n",
    "You are getting \"fims_suffix\" (or similar placeholder text) instead of an actual document reference in cell 17 because the chain and prompt do not explicitly instruct the language model to include the metadata (such as document title or chunk number) from the retrieved documents in the context passed to the LLM.\n",
    "\n",
    "By default, the retriever returns only the text content, not the metadata. If you want the LLM to cite sources, you need to:\n",
    "1. Modify the prompt so it includes metadata (like `doc_title` and `chunk_num`) in the context.\n",
    "2. Adjust how the context is constructed so that each chunk includes its source information.\n",
    "\n",
    "To fix this, update the code that builds the context for the prompt so it formats each chunk with its metadata, for example:\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata.get('doc_title', 'unknown')} (chunk {doc.metadata.get('chunk_num', '?')}):\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "# Then, in your chain, use a RunnableLambda to format the context:\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(format_docs),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "This way, the LLM will see the source information in the context and can cite it in its answer. If you want, I can update your notebook to implement this fix.\n",
    "\n",
    "\"fim_suffix\" is not a standard output or variable from LangChain, OpenAI, or your notebook code. It is likely a hallucinated or placeholder string generated by the language model (LLM) when it does not have access to real citation metadata in the context.\n",
    "\n",
    "This happens because:\n",
    "\n",
    "The LLM is prompted to \"Cite your sources,\" but the context it receives does not include explicit source information (like document titles or chunk numbers).\n",
    "Without this information, the LLM may invent a placeholder like \"fim_suffix\" or similar, as it tries to fulfill the instruction to cite a source.\n",
    "To get real citations, you must format the context passed to the LLM to include the actual metadata (such as document title and chunk number) for each chunk. Otherwise, the LLM will not know the true source and may generate made-up references."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ch1 (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
