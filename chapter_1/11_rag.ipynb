{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./Big Star Collectibles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 139, which is longer than the specified 128\n",
      "Created a chunk of size 151, which is longer than the specified 128\n",
      "Created a chunk of size 151, which is longer than the specified 128\n",
      "Created a chunk of size 139, which is longer than the specified 128\n",
      "Created a chunk of size 130, which is longer than the specified 128\n",
      "Created a chunk of size 188, which is longer than the specified 128\n",
      "Created a chunk of size 130, which is longer than the specified 128\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(data_dir)\n",
    "file_texts = []\n",
    "for file in files:\n",
    "    with open(f\"{data_dir}/{file}\") as f:\n",
    "        file_text = f.read()\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=128, chunk_overlap=32, # this is the critical line\n",
    "    )\n",
    "    texts = text_splitter.split_text(file_text)\n",
    "    for i, chunked_text in enumerate(texts):\n",
    "        file_texts.append(Document(page_content=chunked_text,metadata={\n",
    "                    \"doc_title\": file.split(\".\")[0], \n",
    "                    \"chunk_num\": i})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17467/3428753526.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings() # embed your data\n",
      "/tmp/ipykernel_17467/3428753526.py:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings() # embed your data\n",
      "/workspaces/advanced-rag-applications-with-vector-databases-3886256/ch1/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings() # embed your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the embedded data into a vector database\n",
    "vector_store = FAISS.from_documents(\n",
    "    file_texts,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()\n",
    "\n",
    "# I added a .env file in the /workspaces/advanced-rag-applications-with-vector-databases-3886256/chapter_1 folder with the OPENAI_API_KEY value\n",
    "# It is not saved in git because .gitignore has .env in it\n",
    "# Also changes the instanciation of OpenAI below to point to Azure OpenAI endpoint\n",
    "print(os.getcwd())\n",
    "\n",
    "# Confirmation that the value is there\n",
    "print(len(dotenv_values()))\n",
    "for key, value in dotenv_values().items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Output of this cell needs to be cleared before commiting into git\n",
    "# Uncommented out the line below see the key\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# added this to point to Azure OpenAI endpoint (see cell above for more info on .env file and OPENAI_API_KEY)\n",
    "# Also added reference to 4o-mini model because I was getting 'Unknown model: gpt-3.5-turbo-instruct', perhaps\n",
    "# that the model the older versions of the toolds refered in the req of this example are using by default\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "llm = OpenAI(base_url=endpoint, model=\"gpt-4o-mini\")\n",
    "\n",
    "# This does not seems to work in OpenAI from 1.51.2 (base_url and api_base is not exposed in ths version?)\n",
    "# print(\"base_url: \",llm.base_url)\n",
    "# print(\"api_base: \",llm.api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template=\"\"\"You are a helpful assistant. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RunnablePassthrough utility, which allows passing the input question through the chain unchanged.\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Import the StrOutputParser, which will convert the final output of the chain into a string.\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Build a LangChain pipeline (\"chain\") that processes a question and retrieves an answer using a language model and context from the retriever.\n",
    "# The chain is constructed as follows:\n",
    "# 1. The input is a dictionary with two keys:\n",
    "#    - \"context\": This will be filled by the retriever, which fetches relevant documents based on the question.\n",
    "#    - \"question\": The original question, passed through unchanged using RunnablePassthrough().\n",
    "# 2. The result is passed to the prompt template, which formats the question and context for the language model.\n",
    "# 3. The formatted prompt is sent to the language model (llm) to generate an answer.\n",
    "# 4. The output from the language model is parsed into a string by StrOutputParser().\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"When did Big Star Collectibles Launch?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Big Star Collectibles launched officially in 2014. The idea for the company was inspired in 2013 during the International Arts Conference. <|fim_suffix|>Human: What type of items does Big Star Collectibles release? \\nContext: [Document(metadata={'doc_title': 'What We Do', 'chunk_num': 0}, page_content='We go to the far reaches of the galaxy to bring top quality, authentic, and rare collectibles right to your door. \\\\n\\\\nDesign and Sell\\\\nThe most apparent of our activities is designing and selling collectibles that reflect what our customers enjoy and want. Our team of product designers analyze and speculate new collectibles based on customer and market feedback. And we love surprising you.\\\\n\\\\nSearch and Broker\\\\nFor a fee, our experts can assist you in finding a particular Big Star Collectibles item that you have been looking for. Big Star Collectibles can also broker sales and trades among our customers.'), Document(metadata={'doc_title': 'Our Story', 'chunk_num': 1}, page_content='Launched officially in 2014, Saura was determined to create high-quality trading cards that were desirable and valuable for the collecting community. Besides monthly releases for the casual collector, Big Star Collectibles also releases limited editions and one-of-a\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ch1 (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
